services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    # 一部環境では deploy.devices が無視されるため、runtime を明示
    runtime: nvidia
    environment:
      - TZ=Asia/Tokyo
      # 軽量運用前提の既定値（必要に応じて調整）
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_KEEP_ALIVE=30m
      # - OLLAMA_HOST=0.0.0.0
      # GPU利用時の推奨（NVIDIA Container Toolkit 前提）
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    volumes:
      # モデルとキャッシュを永続化（プロジェクト内に保存）
      - ./ollama-data:/root/.ollama
      # カスタムGGUFなどを配置するローカルディレクトリ（Modelfile から /models を参照）
      - ./models:/models:ro
    ports:
      # ローカル確認用。外出し不要なら削除可
      - "11434:11434"
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:11434/api/tags >/dev/null 2>&1 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 15
    # GPU割当（Compose互換の記法）
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: ["gpu"]
    # mlock の許可（任意）
    cap_add:
      - IPC_LOCK
    ulimits:
      memlock:
        soft: -1
        hard: -1
    networks:
      - edge
    restart: unless-stopped

  # 任意：Ollamaを単体公開する Cloudflare Tunnel（別トークンを推奨）
  cloudflared-ollama:
    image: cloudflare/cloudflared:latest
    depends_on:
      - ollama
    command: ["tunnel","--no-autoupdate","run","--token","${CF_TUNNEL_TOKEN_OLLAMA}"]
    networks:
      - edge
    restart: unless-stopped

networks:
  edge:
    external: true
    name: edge
